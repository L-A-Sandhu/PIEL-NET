{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11e9bf55",
   "metadata": {},
   "source": [
    "\n",
    "# PIEL-NET Pipeline (Notebook)\n",
    "This notebook mirrors `main.py` and runs the end-to-end PIEL-NET pipeline for city–region temperature forecasting.\n",
    "- Physics prior (advection–diffusion) → LMS baseline → hybrid alignment  \n",
    "- ConvLSTM baseline (V4) and RAFL-specialized expert (V5)  \n",
    "- edRVFL-SC ensemble fusion → save final metrics and predictions\n",
    "\n",
    "> **Note:** Ensure your environment can import `ed_rvfl_sc`. If you package it as a module, place it on `PYTHONPATH` or next to this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5a331c",
   "metadata": {},
   "source": [
    "\n",
    "## 0) (Optional) Install dependencies\n",
    "If you already set up a virtualenv with `requirements.txt`, skip this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8ad2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt\n",
    "# Or install manually:\n",
    "# !pip install numpy pandas scipy matplotlib scikit-learn tensorflow keras tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92db6bb",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Imports and path setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d45a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, json, sys, numpy as np\n",
    "\n",
    "# Make sure local src/ is importable if you keep modules there\n",
    "sys.path.insert(0, './src')\n",
    "\n",
    "from data_loader import load_data\n",
    "from advection import get_optimized_physics_predictions\n",
    "from matric import calculate_metrics\n",
    "from data_transform import transform_X\n",
    "from error_compute import calculate_combined_errors\n",
    "from extract_target_column import extract_target_column\n",
    "from fuzzy_mem import compute_fuzzy_memberships\n",
    "from PIEL_NET import HybridModel\n",
    "\n",
    "# Optional expert: ensure this is installed or available on path\n",
    "from ed_rvfl_sc import edRVFL_SC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a50624e",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Parameters (replace argparse)\n",
    "Adjust paths and hyperparameters below. The defaults match `main.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b1806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    # Data + IO\n",
    "    data_dir = \"./Data\",\n",
    "    results_root = \"Results/Ablation\",\n",
    "    output_name = \"C_T2M\",\n",
    "    # Sequence config\n",
    "    T = 48,  # input window\n",
    "    S = 12,  # stride\n",
    "    H = 12,  # horizon\n",
    "    # Training\n",
    "    epochs = 1000,\n",
    "    batch_size = 32,\n",
    "    patience = 10,\n",
    "    # Loss config\n",
    "    loss_type_v4 = \"MAE\",         # [\"MAE\", \"MSE\", \"focal\"]\n",
    "    loss_type_v5 = \"focal\",       # [\"MAE\", \"MSE\", \"focal\"]\n",
    "    alpha_w = 0.2,\n",
    "    beta_w = 2.0,\n",
    "    gamma_w = 5.0,\n",
    "    eta = 0.1,\n",
    "    focal_gamma = 5.0,\n",
    "    # edRVFL-SC\n",
    "    rvfl_units = 256,\n",
    "    rvfl_lambda = 1e-3,\n",
    "    rvfl_Lmax = 7,\n",
    "    rvfl_deep_boost = 0.9,\n",
    "    # Misc\n",
    "    seed = 42\n",
    ")\n",
    "np.random.seed(args.seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33162fcd",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563c4a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c51bbf",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Run the pipeline\n",
    "This cell processes all `.csv` files in `args.data_dir` and saves only the **final** ensemble results under `Results/<dataset>/PIELNET/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb094af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create results root\n",
    "ensure_dir(args.results_root)\n",
    "\n",
    "for csv_file in sorted(os.listdir(args.data_dir)):\n",
    "    if not csv_file.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    csv_path = os.path.join(args.data_dir, csv_file)\n",
    "    dataset = os.path.splitext(csv_file)[0]\n",
    "    run_dir = ensure_dir(os.path.join(args.results_root, dataset))\n",
    "    final_dir = ensure_dir(os.path.join(run_dir, \"PIELNET\"))  # unified folder name\n",
    "\n",
    "    print(f\"\\n=== Processing dataset: {dataset} ===\")\n",
    "\n",
    "    # ---- Load & window ----\n",
    "    X_train, Y_train, X_val, Y_val, X_test, Y_test, norm_stats, col_map = load_data(\n",
    "        csv_path=csv_path, T=args.T, S=args.S, H=args.H, output_dir=None\n",
    "    )\n",
    "\n",
    "    # ---- Physics predictions ----\n",
    "    Y_train_phy, Y_val_phy, Y_test_phy, _ = get_optimized_physics_predictions(\n",
    "        X_train, Y_train, X_val, X_test, norm_stats, col_map, args.H\n",
    "    )\n",
    "\n",
    "    # Align physics predictions via linear map\n",
    "    A = np.linalg.pinv(Y_train_phy) @ Y_train\n",
    "    Y_train_phy = Y_train_phy @ A\n",
    "    Y_val_phy   = Y_val_phy   @ A\n",
    "    Y_test_phy  = Y_test_phy  @ A\n",
    "\n",
    "    # ---- LMS baseline over raw features ----\n",
    "    Xtr = X_train.reshape(X_train.shape[0], -1)\n",
    "    Xva = X_val.reshape(X_val.shape[0], -1)\n",
    "    Xte = X_test.reshape(X_test.shape[0], -1)\n",
    "    W_lms, *_ = np.linalg.lstsq(Xtr, Y_train, rcond=None)\n",
    "    Y_est_train = Xtr @ W_lms\n",
    "    Y_est_val   = Xva @ W_lms\n",
    "    Y_est_test  = Xte @ W_lms\n",
    "\n",
    "    # ---- Physics + LMS hybrid projection ----\n",
    "    Y_train_H = np.hstack([Y_est_train, Y_train_phy])\n",
    "    Y_val_H   = np.hstack([Y_est_val,   Y_val_phy])\n",
    "    Y_test_H  = np.hstack([Y_est_test,  Y_test_phy])\n",
    "\n",
    "    A2 = np.linalg.pinv(Y_train_H) @ Y_train\n",
    "    Y_train_H = Y_train_H @ A2\n",
    "    Y_val_H   = Y_val_H   @ A2\n",
    "    Y_test_H  = Y_test_H  @ A2\n",
    "\n",
    "    # ---- Error signals & memberships ----\n",
    "    Err_train = calculate_combined_errors(Y_train_H, Y_train)\n",
    "    Err_val   = calculate_combined_errors(Y_val_H,   Y_val)\n",
    "\n",
    "    # Not used later but available if needed (kept for parity with main.py)\n",
    "    _ = extract_target_column(X_train, col_map, target_col=args.output_name)\n",
    "\n",
    "    train_memberships, val_memberships = compute_fuzzy_memberships(\n",
    "        Err_train, Err_val, mf_type=\"triangle\"\n",
    "    )\n",
    "\n",
    "    # ---- Data transform for ConvLSTM model ----\n",
    "    X_train_P = np.expand_dims(transform_X(X_train, col_map)[0], axis=-1)\n",
    "    X_val_P   = np.expand_dims(transform_X(X_val,   col_map)[0], axis=-1)\n",
    "    X_test_P  = np.expand_dims(transform_X(X_test,  col_map)[0], axis=-1)\n",
    "\n",
    "    # Pack labels with error + memberships\n",
    "    YY_train = np.column_stack((Y_train, Err_train, train_memberships))\n",
    "    YY_val   = np.column_stack((Y_val,   Err_val,   val_memberships))\n",
    "\n",
    "    # ---- Data model V4 ----\n",
    "    data_model = HybridModel(\n",
    "        input_shape=X_train_P.shape[1:],\n",
    "        pi_dim=Y_train.shape[1],\n",
    "        checkpoint_path=os.path.join(run_dir, \"_tmp_ignore_v4\"),\n",
    "        loss_type=args.loss_type_v4,\n",
    "        alpha=args.alpha_w, beta=args.beta_w, gamma=args.gamma_w,\n",
    "        eta=args.eta, focal_gamma=args.focal_gamma\n",
    "    )\n",
    "    data_model.fit(\n",
    "        X_train_P, YY_train,\n",
    "        validation_data=(X_val_P, YY_val),\n",
    "        epochs=args.epochs, batch_size=args.batch_size, patience=args.patience\n",
    "    )\n",
    "    Y_train_D = data_model.predict(X_train_P)\n",
    "    Y_val_D   = data_model.predict(X_val_P)\n",
    "    Y_test_D  = data_model.predict(X_test_P)\n",
    "\n",
    "    # ---- Data model V5 (RAFL) ----\n",
    "    data_model_F = HybridModel(\n",
    "        input_shape=X_train_P.shape[1:],\n",
    "        pi_dim=Y_train.shape[1],\n",
    "        checkpoint_path=os.path.join(run_dir, \"_tmp_ignore_v5\"),\n",
    "        loss_type=args.loss_type_v5,\n",
    "        alpha=args.alpha_w, beta=args.beta_w, gamma=args.gamma_w,\n",
    "        eta=args.eta, focal_gamma=args.focal_gamma\n",
    "    )\n",
    "    data_model_F.fit(\n",
    "        X_train_P, YY_train,\n",
    "        validation_data=(X_val_P, YY_val),\n",
    "        epochs=args.epochs, batch_size=args.batch_size, patience=args.patience\n",
    "    )\n",
    "    Y_train_FD = data_model_F.predict(X_train_P)\n",
    "    Y_val_FD   = data_model_F.predict(X_val_P)\n",
    "    Y_test_FD  = data_model_F.predict(X_test_P)\n",
    "\n",
    "    # ---- Final ensemble (edRVFL-SC) ----\n",
    "    Z_train = np.hstack([Y_train_D, Y_train_FD, Y_train_H])\n",
    "    Z_val   = np.hstack([Y_val_D,   Y_val_FD,   Y_val_H])\n",
    "    Z_test  = np.hstack([Y_test_D,  Y_test_FD,  Y_test_H])\n",
    "\n",
    "    MOE = edRVFL_SC(\n",
    "        num_units=args.rvfl_units,\n",
    "        activation=\"relu\",\n",
    "        lambda_=args.rvfl_lambda,\n",
    "        Lmax=args.rvfl_Lmax,\n",
    "        deep_boosting=args.rvfl_deep_boost\n",
    "    )\n",
    "    MOE.train(Z_train, Y_train)\n",
    "    Y_test_final = MOE.predict(Z_test)\n",
    "\n",
    "    # ---- Persist only final metrics under PIELNET ----\n",
    "    metrics_all = {}\n",
    "    metrics_all[\"PIELNET\"] = calculate_metrics(\n",
    "        Y_test, Y_test_final, args.output_name, norm_stats, final_dir\n",
    "    )\n",
    "\n",
    "    # Also save a compact metrics.json at dataset root for quick scan\n",
    "    with open(os.path.join(run_dir, \"metrics.json\"), \"w\") as f:\n",
    "        json.dump(metrics_all, f, indent=4)\n",
    "\n",
    "    print(f\"✓ Completed {dataset} → {final_dir}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}